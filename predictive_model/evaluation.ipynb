{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe1748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation.py\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def eval_model(y_true, y_pred, group_column=None, X=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "    FPR = FP / (FP + TN)\n",
    "    FNR = FN / (FN + TP)\n",
    "\n",
    "    results = {\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"FPR\": FPR,\n",
    "        \"FNR\": FNR\n",
    "    }\n",
    "\n",
    "    # Optional fairness check\n",
    "    if group_column is not None and X is not None:\n",
    "        fairness = {}\n",
    "        for g in X[group_column].unique():\n",
    "            idx = X[group_column] == g\n",
    "            cm_g = confusion_matrix(y_true[idx], y_pred[idx]).ravel()\n",
    "            tng, fpg, fng, tpg = cm_g\n",
    "            fairness[g] = {\n",
    "                \"FPR\": fpg / (fpg + tng) if (fpg+tng)>0 else None,\n",
    "                \"FNR\": fng / (fng + tpg) if (fng+tpg)>0 else None\n",
    "            }\n",
    "        results[\"fairness\"] = fairness\n",
    "\n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
